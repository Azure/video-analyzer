# Pipeline Topologies

Each sub-folder in this directory contains the JSON file, description, and diagram for a live pipeline topology. The following table also lists each topology along with a short description, its corresponding sample tutorial(s), and its corresponding pipeline topology name on the [Azure Video Analyzer extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.azure-video-analyzer) on Visual Studio Code (VSCode). Clicking on a topology name redirects to the corresponding JSON file.

## Continuous Video Recording
Name | Description | Sample Tutorials | VSCode Name
:----- | :----  | :---- | :---
[cvr-video-sink](cvr-video-sink\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. | [Continuous video recording and playback](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/use-continuous-video-recording) | Continuous Video Recording > Record to Video Analyzer video
[cvr-with-grpcExtension](cvr-with-grpcExtension\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, a subset of the video frames are sent to an external AI inference engine using the sharedMemory mode for data transfer via the gRPC extension. The results are then published to the IoT Edge Hub. | | Continuous Video Recording > Record using gRPC Extension
[cvr-with-httpExtension](cvr-with-httpExtension\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, a subset of the video frames are sent to an external AI inference engine via the HTTP extension. The results are then published to the IoT Edge Hub. | | Continuous Video Recording > Record using HTTP Extension
[cvr-with-httpExtension-and-objectTracking](cvr-with-httpExtension-and-objectTracking\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, track objects in a live feed. The object tracker comes in handy when you need to detect objects in every frame, but the edge device does not have the necessary compute power to be able to apply the vision model on every frame. The object tracker will also send the inference metadata to the video sink to be recorded and played back with the video. | [Record and stream inference metadata with video](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/record-stream-inference-data-with-video) 
[cvr-with-motion](cvr-with-motion\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, the video from the camera is analyzed for the presence of motion. When motion is detected from a live video feed, relevant inferencing events are published to the IoT Edge Hub. | | Continuous Video Recording > Record on motion detection
[audio-video](audio-video\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, record audio using the **outputSelectors** property. | | Continuous Video Recording > Record audio with video

## Event-based Video Recording
Name | Description | Sample Tutorials | VSCode Name
:----- | :----  | :---- | :---
[evr-grpcExtension-video-sink](evr-grpcExtension-video-sink\topology.json)  | Perform event-based recording. When an event of interest is detected by the external AI inference engine via the gRPC extension, those events are published to the IoT Edge Hub. In addition, the events are used to trigger the signal gate processor node which results in the appending of new clips to the Azure Video Analyzer video, corresponding to when the event of interest was detected. | [Develop and deploy gRPC inference server](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/develop-deploy-grpc-inference-srv) | Event-based Video Recording > Record using gRPC Extension
[evr-httpExtension-video-sink](evr-httpExtension-video-sink\topology.json) | Perform event-based recording. When an event of interest is detected by the external AI inference engine via the HTTP extension, those events are published to the IoT Edge Hub. In addition, the events are used to trigger the signal gate processor node which results in the appending of new clips to the Azure Video Analyzer video, corresponding to when the event of interest was detected. | | Event-based Video Recording > Record using HTTP Extension
[evr-hubMessage-video-sink](evr-hubMessage-video-sink\topology.json) | Use an object detection AI model to look for objects in the video, and record video clips only when a certain type of object is detected. The trigger to generate these clips is based on the AI inference events published onto the IoT Hub. | [Event-based video recording and playback](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/record-event-based-live-video)| Event-based Video Recording > Record to Video Analyzer video based on inference events
[evr-hubMessage-file-sink](evr-hubMessage-file-sink\topology.json) | Record video clips to the local file system of the edge device whenever an external sensor sends a message to the pipeline topology. For example, the sensor can be a door sensor. | | Event-based Video Recording > Record to local files based on inference events
[evr-motion-video-sink-file-sink](evr-motion-video-sink-file-sink\topology.json) | Perform event-based recording of video clips to the cloud as well as to the edge. When motion is detected from a live video feed, events are sent to a signal gate processor node which opens, allowing video to pass through to a file sink node as well as an video sink node. As a result, new files are created on the local file system of the Edge device, and new video clips are appended to your Video Analyzer video. The recordings contain the frames where motion was detected. | | Event-based Video Recording > Record motion events to Video Analyzer video and local files 
[evr-motion-video-sink](evr-motion-video-sink\topology.json) | Perform event-based recording. When motion is detected, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger the signal gate processor node which will send frames to the video sink node when motion is detected. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. | [Detect motion, record video to Video Analyzer](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/detect-motion-record-video-clips-cloud) | Event-based Video Recording > Record motion events to Video Analyzer video
[evr-motion-file-sink](evr-motion-file-sink\topology.json) | Perform event-based recording. When motion is detected from a live video feed, events are sent to a signal gate processor node which opens, sending frames to a file sink node. As a result, new files are created on the local file system of the Edge device, containing the frames where motion was detected. | [Detect motion and record video on edge devices](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/detect-motion-record-video-edge-devices?pivots=programming-language-csharp) | Event-based Video Recording > Record motion events to local files 

## Motion Detection
Name | Description | Sample Tutorials | VSCode Name
:----- | :----  | :---- | :---
[motion-detection](motion-detection\topology.json) | Detect motion in a live video feed. When motion is detected, those events are published to the IoT Hub. | [Get started with Azure Video Analyzer](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/get-started-detect-motion-emit-events), [Get started with Video Analyzer in the portal](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/get-started-detect-motion-emit-events-portal), [Detect motion and emit events](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/detect-motion-emit-events-quickstart) | Motion Detection > Publish motion events to IoT Hub
[motion-with-grpcExtension](motion-with-grpcExtension\topology.json) | Perform event-based recording in the presence of motion. When motion is detected from a live video feed, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger a signal gate processor node which will send frames to an video sink node only when motion is present. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. Additionally, run video analytics only when motion is detected. Upon detecting motion, a subset of the video frames are sent to an external AI inference engine via the gRPC extension. The results are then published to the IoT Edge Hub. | [Analyze live video with your own model - gRPC](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/analyze-live-video-use-your-model-grpc?pivots=programming-language-csharp) | Motion Detection > Publish motion events using gRPC Extension
[motion-with-httpExtension](motion-with-httpExtension\topology.json) | Perform event-based recording in the presence of motion. When motion is detected in a live video feed, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger a signal gate processor node which will send frames to a video sink node only when motion is present. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. Additionally, run video analytics only when motion is detected. Upon detecting motion, a subset of the video frames are sent to an external AI inference engine via the HTTP extension. The results are then published to the IoT Edge Hub. | [Analyze live video with your own model - HTTP](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/analyze-live-video-use-your-model-http?pivots=programming-language-csharp#generate-and-deploy-the-iot-edge-deployment-manifest) | Motion Detection > Publish motion events using HTTP Extension

## Extensions
Name | Description | Sample Tutorials | VSCode Name
:----- | :----  | :---- | :---
[grpcExtensionOpenVINO](grpcExtensionOpenVINO\topology.json) | Run video analytics on a live video feed. The gRPC extension allows you to create images at video frame rate from the camera that are converted to images, and sent to the [OpenVINO™ DL Streamer - Edge AI Extension module](https://aka.ms/ava-intel-ovms) provided by Intel. The results are then published to the IoT Edge Hub. | [Analyze live video with Intel OpenVINO™ DL Streamer – Edge AI Extension](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/use-intel-grpc-video-analytics-serving-tutorial)
[httpExtension](httpExtension\topology.json) | Run video analytics on a live video feed. A subset of the video frames from the camera are converted to images, and sent to an external AI inference engine. The results are then published to the IoT Edge Hub. | [Analyze live video with your own model - HTTP](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/analyze-live-video-use-your-model-http), [Analyze live video with Azure Video Analyzer on IoT Edge and Azure Custom Vision](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/analyze-live-video-custom-vision?pivots=programming-language-csharp) | Extensions > Analyzer video using HTTP Extension
[httpExtensionOpenVINO](httpExtensionOpenVINO\topology.json) | Run video analytics on a live video feed. A subset of the video frames from the camera are converted to images, and sent to the [OpenVINO™ Model Server – AI Extension module](https://aka.ms/ava-intel-ovms) provided by Intel. The results are then published to the IoT Edge Hub. | [Analyze live video using OpenVINO™ Model Server – AI Extension from Intel](https://aka.ms/ava-intel-ovms-tutorial) | Extensions > Analyzer video with Intel OpenVINO Model Server

## Computer Vision
Name | Description | Sample Tutorials | VSCode Name
:----- | :----  | :---- | :---
[spatial-analysis/person-count-operation-topology](spatial-analysis\person-count-operation-topology.json) | Live video is sent to an external [spatialAnalysis](https://docs.microsoft.com/azure/cognitive-services/computer-vision/spatial-analysis-operations) module which counts people in a designated zone. When the criteria defined by the AI operation is met, events are sent to a signal gate processor which opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | | Computer Vision > Person count operation with Computer Vision for Spatial Analysis
[spatial-analysis/person-line-crossing-operation-topology](spatial-analysis\person-line-crossing-operation-topology.json) | Live video is sent to an external [spatialAnalysis](https://docs.microsoft.com/azure/cognitive-services/computer-vision/spatial-analysis-operations) module which tracks when a person crosses a designated line. When the criteria defined by the AI operation is met, events are sent to a signal gate processor which opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | | Computer Vision > Person crossing line operation with Computer Vision for Spatial Analysis
[spatial-analysis/person-zone-crossing-operation-topology](spatial-analysis\person-zone-crossing-operation-topology.json) | Live video is sent to an external [spatialAnalysis](https://docs.microsoft.com/azure/cognitive-services/computer-vision/spatial-analysis-operations) module which emits an event when a person enters or exists a zone. When the criteria defined by the AI operation is met, events are sent to a signal gate processor which opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | [Live Video with Computer Vision for Spatial Analysis](https://aka.ms/ava-spatial-analysis) | Computer Vision > Person crossing zone operation with Computer Vision for Spatial Analysis
[spatial-analysis/person-distance-operation-topology](spatial-analysis\person-distance-operation-topology.json) | Live video is sent to an external [spatialAnalysis](https://docs.microsoft.com/azure/cognitive-services/computer-vision/spatial-analysis-operations) module which tracks when people violate a distance rule. When the criteria defined by the AI operation is met, events are sent to a signal gate processor which opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | | Computer Vision > Person distance operation with Computer Vision for Spatial Analysis
[spatial-analysis/custom-operation-topology](spatial-analysis\custom-operation-topology.json) | Live video is sent to an external [spatialAnalysis](https://docs.microsoft.com/azure/cognitive-services/computer-vision/spatial-analysis-operations) module which carries out a supported AI operation. When the criteria defined by the AI operation is met, events are sent to a signal gate processor which opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | | Computer Vision > Custom operation with Computer Vision for Spatial Analysis

## AI Composition
Name | Description | Sample Tutorials | VSCode Name
:----- | :----  | :---- | :---
[ai-composition](ai-composition/topology.json) | Run 2 AI inferencing models of your choice. In this example, classified video frames are sent from an AI inference engine using the [Tiny YOLOv3 model](https://github.com/Azure/video-analyzer/tree/main/edge-modules/extensions/yolo/tinyyolov3/grpc-cpu) to another engine using the [YOLOv3 model](https://github.com/Azure/video-analyzer/tree/main/edge-modules/extensions/yolo/yolov3/grpc-cpu). Having such a topology enables you to trigger a heavy AI module, only when a light AI module indicates a need to do so. | [Analyze live video streams with multiple AI models using AI composition](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/analyze-ai-composition) | AI Composition > Record to AMS asset using multiple AI models

## Miscellaneous
Name | Description | Sample Tutorials | VSCode Name
:----- | :----  | :---- | :---
[object-tracking](object-tracking\topology.json) | Track objects in a live video feed. The object tracker comes in handy when you need to detect objects in every frame, but the edge device does not have the necessary compute power to be able to apply the vision model on every frame. | [Track objects in a live video](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/track-objects-live-video) | Miscellaneous > Record video based on the object tracking AI model 
[line-crossing](line-crossing\topology.json) | Use a computer vision model to detect objects in a subset of frames when they cross a virtual line in a live video feed. The object tracker node is used to track those objects in the frames and pass them through a line crossing node. The line crossing node comes in handy when you want to detect objects that cross the imaginary line and emit events. | [Detect when objects cross a virtual line in a live video](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/use-line-crossing) | Miscellaneous > Record video based on the line crossing AI model