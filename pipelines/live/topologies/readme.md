# Pipeline Topologies

Each sub-folder in this directory contains the JSON file, description, and diagram for a live pipeline topology. The following table also lists each topology along with a short description. Clicking on a topology name redirects to the corresponding JSON file.

Name | Description | Sample Tutorial
:----- | :----  | :----
[ai-composition](ai-composition/topology.json) | Run 2 AI inferencing models of your choice. In this example, classified video frames are sent from an AI inference engine using the [Tiny YOLOv3 model](https://github.com/Azure/video-analyzer/tree/main/edge-modules/extensions/yolo/tinyyolov3/grpc-cpu) to another engine using the [YOLOv3 model](https://github.com/Azure/video-analyzer/tree/main/edge-modules/extensions/yolo/yolov3/grpc-cpu). Having such a topology enables you to trigger a heavy AI module, only when a light AI module indicates a need to do so. | [Analyze live video streams with multiple AI models using AI composition](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/analyze-ai-composition)
[cvr-video-sink](cvr-video-sink\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. | [Continuous video recording and playback](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/use-continuous-video-recording)
[cvr-with-grpcExtension](cvr-with-grpcExtension\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, a subset of the video frames are sent to an external AI inference engine using the sharedMemory mode for data transfer via the gRPC extension. The results are then published to the IoT Edge Hub.
[cvr-with-httpExtension](cvr-with-httpExtension\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, a subset of the video frames are sent to an external AI inference engine via the HTTP extension. The results are then published to the IoT Edge Hub.
[cvr-with-httpExtension-and-objectTracking](cvr-with-httpExtension-and-objectTracking\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, track objects in a live feed. The object tracker comes in handy when you need to detect objects in every frame, but the edge device does not have the necessary compute power to be able to apply the vision model on every frame. The object tracker will also send the inference metadata to the video sink to be recorded and played back with the video. | [Record and stream inference metadata with video](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/record-stream-inference-data-with-video)
[cvr-with-motion](cvr-with-motion\topology.json) | Capture video and continuously record it to an Azure Video Analyzer video. Additionally, the video from the camera is analyzed for the presence of motion. When motion is detected from a live video feed, relevant inferencing events are published to the IoT Edge Hub.
[evr-grpcExtension-video-sink](evr-grpcExtension-video-sink\topology.json)  | Perform event-based recording. When an event of interest is detected by the external AI inference engine via the gRPC extension, those events are published to the IoT Edge Hub. In addition, the events are used to trigger the signal gate processor node which results in the appending of new clips to the Azure Video Analyzer video, corresponding to when the event of interest was detected.
[evr-httpExtension-video-sink](evr-httpExtension-video-sink\topology.json) | Perform event-based recording. When an event of interest is detected by the external AI inference engine via the HTTP extension, those events are published to the IoT Edge Hub. In addition, the events are used to trigger the signal gate processor node which results in the appending of new clips to the Azure Video Analyzer video, corresponding to when the event of interest was detected.
[evr-hubMessage-file-sink](evr-hubMessage-file-sink\topology.json) | Record video clips to the local file system of the edge device whenever an external sensor sends a message to the pipeline topology. The sensor, for example, can be a door sensor.
[evr-hubMessage-video-sink](evr-hubMessage-video-sink\topology.json) | Use an object detection AI model to look for objects in the video, and record video clips only when a certain type of object is detected. The trigger to generate these clips is based on the AI inference events published onto the IoT Hub. | [Event-based video recording and playback](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/record-event-based-live-video)
[evr-motion-file-sink](evr-motion-file-sink\topology.json) | Perform event-based recording. When motion is detected from a live video feed, events are sent to a signal gate processor node which opens, sending frames to a file sink node. As a result, new files are created on the local file system of the Edge device, containing the frames where motion was detected. | [Detect motion and record video on edge devices](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/detect-motion-record-video-edge-devices?pivots=programming-language-csharp)
[evr-motion-video-sink](evr-motion-video-sink\topology.json) | Perform event-based recording. When motion is detected, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger the signal gate processor node which will send frames to the video sink node when motion is detected. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. | [Detect motion, record video to Video Analyzer](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/edge/detect-motion-record-video-clips-cloud)
[evr-motion-video-sink-file-sink](evr-motion-video-sink-file-sink\topology.json) | Perform event-based recording of video clips to the cloud as well as to the edge. When motion is detected from a live video feed, events are sent to a signal gate processor node which opens, allowing video to pass through to a file sink node as well as an video sink node. As a result, new files are created on the local file system of the Edge device, and new video clips are appended to your Video Analyzer video. The recordings contain the frames where motion was detected. 
[grpcExtensionOpenVINO](grpcExtensionOpenVINO\topology.json) | Run video analytics on a live video feed. The gRPC extension allows you to create images at video frame rate from the camera that are converted to images, and sent to the [OpenVINO™ DL Streamer - Edge AI Extension module from Intel](https://aka.ms/ava-intel-ovms) module. The results are then published to the IoT Edge Hub. | [Analyze live video with Intel OpenVINO™ DL Streamer – Edge AI Extension](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/use-intel-grpc-video-analytics-serving-tutorial).
[httpExtension](httpExtension\topology.json) | Run video analytics on a live video feed. A subset of the video frames from the camera are converted to images, and sent to an external AI inference engine. The results are then published to the IoT Edge Hub. | [Analyze live video with your own model - HTTP](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/analyze-live-video-use-your-model-http) 
[httpExtensionOpenVINO](httpExtensionOpenVINO\topology.json) | Run video analytics on a live video feed. A subset of the video frames from the camera are converted to images, and sent to the [OpenVINO™ Model Server – AI Extension](https://aka.ms/ava-intel-ovms) module provided by Intel. The results are then published to the IoT Edge Hub. | [Analyze live video using OpenVINO™ Model Server – AI Extension from Intel](https://aka.ms/ava-intel-ovms-tutorial)
[line-crossing](line-crossing\topology.json) | Use a computer vision model to detect objects in a subset of frames when they cross a virtual line in a live video feed. The object tracker node is used to track those objects in the frames and pass them through a line crossing node. The line crossing node comes in handy when you want to detect objects that cross the imaginary line and emit events. | [Detect when objects cross a virtual line in a live video](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/use-line-crossing) 
[motion-detection](motion-detection\topology.json) | Detect motion in a live video feed. When motion is detected, those events are published to the IoT Hub. | [Detect motion and emit events](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/detect-motion-emit-events-quickstart) 
[motion-with-grpcExtension](motion-with-grpcExtension\topology.json) | Perform event-based recording in the presence of motoin. When motion is detected from a live video feed, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger a signal gate processor node which will send frames to an video sink node only when motion is present. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. Additionally, run video analytics only when motion is detected. Upon detecting motion, a subset of the video frames are sent to an external AI inference engine via the gRPC extension. The results are then published to the IoT Edge Hub. | [Analyze live video with your own model - gRPC](https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-docs/analyze-live-video-use-your-model-grpc?pivots=programming-language-csharp) 
[motion-with-httpExtension](motion-with-httpExtension\topology.json) | Perform event-based recording in the presence of motion. When motion is detected in a live video feed, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger a signal gate processor node which will send frames to a video sink node only when motion is present. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. Additionally, run video analytics only when motion is detected. Upon detecting motion, a subset of the video frames are sent to an external AI inference engine via the HTTP extension. The results are then published to the IoT Edge Hub.
[object-tracking](object-tracking\topology.json) | Track objects in a live video feed. The object tracker comes in handy when you need to detect objects in every frame, but the edge device does not have the necessary compute power to be able to apply the vision model on every frame. | [Track objects in a live video](https://docs.microsoft.com/azure/azure-video-analyzer/video-analyzer-docs/track-objects-live-video) 
[spatial-analysis](spatial-analysis\topology.json) | Live video is sent to an external [spatialAnalysis](https://docs.microsoft.com/azure/cognitive-services/computer-vision/spatial-analysis-operations) module which carries out a supported AI operation. When the criteria defined by the AI operation is met, events are sent to a signal gate processor which opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | [Live Video with Computer Vision for Spatial Analysis](https://aka.ms/ava-spatial-analysis) 
